{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "collapsed": false,
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "from numpy import shape, matrix, log, exp, zeros,random,dot,multiply\n",
    "from mnist import readWithoutBias\n",
    "from math import sqrt"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "collapsed": false,
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "data_train, label_train = readWithoutBias(dataset=\"training\")\n",
    "data_test, label_test = readWithoutBias(dataset=\"testing\")\n",
    "label_train = matrix(label_train)\n",
    "label_test  = matrix(label_test)\n",
    "data_train  = matrix(data_train).T\n",
    "data_test   = matrix(data_test).T"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "collapsed": false,
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Users/apoorve/anaconda/lib/python2.7/site-packages/IPython/kernel/__main__.py:3: RuntimeWarning: invalid value encountered in divide\n",
      "  app.launch_new_instance()\n",
      "/Users/apoorve/anaconda/lib/python2.7/site-packages/IPython/kernel/__main__.py:4: RuntimeWarning: divide by zero encountered in divide\n",
      "/Users/apoorve/anaconda/lib/python2.7/site-packages/IPython/kernel/__main__.py:4: RuntimeWarning: invalid value encountered in divide\n"
     ]
    }
   ],
   "source": [
    "train_mean  = data_train.mean(axis=1)\n",
    "train_std   = data_train.std(axis=1)\n",
    "data_train  = np.nan_to_num((data_train - train_mean)/train_std)\n",
    "data_test   = np.nan_to_num((data_test - train_mean)/train_std)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {
    "collapsed": false,
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "X_test = data_test\n",
    "y_test = label_test"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##Initializations"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {
    "collapsed": false,
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "train = zip(data_train.T,label_train)\n",
    "mini_batch_size = 10\n",
    "\n",
    "n_input = shape(data_train)[0] # excluding bias term\n",
    "n_hidden = 30               # excluding bias term\n",
    "n_output = 10\n",
    "\n",
    "theta1 = matrix(random.randn(n_hidden,n_input))/sqrt(n_input)\n",
    "bias1  = matrix(random.randn(n_hidden,1))\n",
    "theta2 = matrix(random.randn(n_output,n_hidden))/sqrt(n_hidden)\n",
    "bias2  = matrix(random.randn(n_output,1))\n",
    "\n",
    "afunc, afuncGradient = act_funcs[\"leaky_relu\"]\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Training Code"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "batch num:  0\n",
      "batch num:  1000\n",
      "batch num:  2000\n",
      "batch num:  3000\n",
      "batch num:  4000\n",
      "batch num:  5000\n",
      "0.098\n",
      "batch num:  0\n",
      "batch num:  1000\n",
      "batch num:  2000\n",
      "batch num:  3000\n",
      "batch num:  4000\n",
      "batch num:  5000\n",
      "0.098\n",
      "batch num:  0\n",
      "batch num:  1000\n",
      "batch num:  2000\n",
      "batch num:  3000\n",
      "batch num:  4000\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Users/apoorve/anaconda/lib/python2.7/site-packages/IPython/kernel/__main__.py:10: RuntimeWarning: invalid value encountered in greater_equal\n",
      "/Users/apoorve/anaconda/lib/python2.7/site-packages/IPython/kernel/__main__.py:10: RuntimeWarning: invalid value encountered in less\n"
     ]
    },
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-32-09670d321860>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m()\u001b[0m\n\u001b[1;32m     13\u001b[0m         \u001b[0mdb2\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mnp\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mzeros_like\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mbias2\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     14\u001b[0m         \u001b[0;32mfor\u001b[0m \u001b[0mX\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0my\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mmini_batch\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 15\u001b[0;31m             \u001b[0mgradTheta1\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mgradBias1\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mgradTheta2\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mgradBias2\u001b[0m \u001b[0;34m=\u001b[0m                 \u001b[0mbackPropGradient\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mX\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mT\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0my\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mtheta1\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mtheta2\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mbias1\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mbias2\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;31m#just one example passed\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     16\u001b[0m             \u001b[0md1\u001b[0m  \u001b[0;34m+=\u001b[0m \u001b[0mgradTheta1\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     17\u001b[0m             \u001b[0mdb1\u001b[0m \u001b[0;34m+=\u001b[0m \u001b[0mgradBias1\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m<ipython-input-5-26312a3feae9>\u001b[0m in \u001b[0;36mbackPropGradient\u001b[0;34m(X, y, theta1, theta2, bias1, bias2)\u001b[0m\n\u001b[1;32m     16\u001b[0m     \u001b[0mgradTheta2\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mdot\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mdelta3\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0ma2\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mT\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     17\u001b[0m     \u001b[0mgradBias2\u001b[0m  \u001b[0;34m=\u001b[0m \u001b[0mdelta3\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 18\u001b[0;31m     \u001b[0mgradTheta1\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mdot\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mdelta2\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0ma1\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mT\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     19\u001b[0m     \u001b[0mgradBias1\u001b[0m  \u001b[0;34m=\u001b[0m \u001b[0mdelta2\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     20\u001b[0m     \u001b[0;32mreturn\u001b[0m \u001b[0mgradTheta1\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mgradBias1\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mgradTheta2\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mgradBias2\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m: "
     ]
    }
   ],
   "source": [
    "epochs = 10\n",
    "count = 0\n",
    "alpha = 0.1\n",
    "mini_batch_size = 10\n",
    "old_accuracy = 0\n",
    "for i in range(epochs):\n",
    "    random.shuffle(train)\n",
    "    mini_batches = [train[k:k+mini_batch_size] for k in xrange(0,len(train),mini_batch_size)]\n",
    "    for mini_batch in mini_batches:\n",
    "        d1  = np.zeros_like(theta1)\n",
    "        d2  = np.zeros_like(theta2)\n",
    "        db1 = np.zeros_like(bias1)\n",
    "        db2 = np.zeros_like(bias2)\n",
    "        for X,y in mini_batch:\n",
    "            gradTheta1, gradBias1, gradTheta2, gradBias2 = \\\n",
    "                backPropGradient(X.T, y, theta1, theta2, bias1, bias2) #just one example passed\n",
    "            d1  += gradTheta1\n",
    "            db1 += gradBias1\n",
    "            d2  += gradTheta2\n",
    "            db2 += gradBias2\n",
    "        d1  /= mini_batch_size\n",
    "        db1 /= mini_batch_size\n",
    "        d2  /= mini_batch_size\n",
    "        db2 /= mini_batch_size\n",
    "        theta1 = theta1 - alpha*d1\n",
    "        bias1  = bias1  - alpha*db1\n",
    "        theta2 = theta2 - alpha*d2\n",
    "        bias2  = bias2  - alpha*db2\n",
    "        if count%1000==0:\n",
    "            print \"batch num: \",count\n",
    "        count+=1\n",
    "    print accuracy(X_test, y_test, theta1, theta2, bias1, bias2)# get error on complete data\n",
    "    count = 0"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Checking backpropgradient vs numerical gradient"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {
    "collapsed": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "-6.03219557638e-11\n",
      "-3.83120515564e-12\n",
      "7.40154326717e-11\n",
      "-1.6722715477e-08\n"
     ]
    }
   ],
   "source": [
    "X, y = train[0]\n",
    "X = X.T\n",
    "gradTheta1, gradBias1, gradTheta2, gradBias2 = backPropGradient(X,y,theta1,theta2, bias1, bias2)\n",
    "numGrad1,numGradBias1,numGrad2,numGradBias2  = numericalGradient(X,y,theta1,theta2, bias1, bias2)\n",
    "print np.sum(np.subtract(numGradBias2,gradBias2))\n",
    "print np.sum(np.subtract(numGradBias1,gradBias1))\n",
    "print np.sum(np.subtract(numGrad2,gradTheta2))\n",
    "print np.sum(np.subtract(numGrad1,gradTheta1))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Helper Functions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "collapsed": true,
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "def accuracy(X, y, theta1, theta2, bias1, bias2):\n",
    "    a1 = X\n",
    "    z2 = dot(theta1,a1) + bias1\n",
    "    a2 = afunc(z2)\n",
    "    z3 = dot(theta2,a2) + bias2\n",
    "    a3 = afunc(z3)\n",
    "    \n",
    "    pred = np.argmax(a3,axis=0)\n",
    "    return np.sum(np.equal(pred,y.T))/float(len(y))\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "collapsed": false,
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "def backPropGradient(X, y, theta1, theta2, bias1, bias2):\n",
    "    a1 = X\n",
    "    z2 = dot(theta1,a1) + bias1\n",
    "    a2 = afunc(z2)\n",
    "    z3 = dot(theta2,a2) + bias2\n",
    "    a3 = afunc(z3)\n",
    "    \n",
    "    t = np.zeros_like(a3)\n",
    "    for i in range(len(y)):\n",
    "        t[y[i],i] = 1\n",
    "    \n",
    "    delta3 = a3-t #shape (10,1)\n",
    "    delta2 = dot(theta2.T,delta3)#shape (100,10)X(10,1) = (100,1)\n",
    "    delta2 = multiply(delta2,afuncGradient(z2))\n",
    "\n",
    "    gradTheta2 = dot(delta3,a2.T)\n",
    "    gradBias2  = delta3\n",
    "    gradTheta1 = dot(delta2,a1.T)\n",
    "    gradBias1  = delta2\n",
    "    return gradTheta1, gradBias1, gradTheta2, gradBias2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {
    "collapsed": true,
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "def errorFn(X, y, theta1, theta2, bias1, bias2):\n",
    "    n_examples = shape(X)[1]\n",
    "    a1 = X\n",
    "    z2 = dot(theta1,a1) + bias1\n",
    "    a2 = afunc(z2)\n",
    "    z3 = dot(theta2,a2) + bias2\n",
    "    a3 = afunc(z3)\n",
    "    \n",
    "    t = np.zeros_like(a3)\n",
    "    for i in range(len(y)):\n",
    "        t[y[i],i] = 1\n",
    "\n",
    "    error = np.sum(multiply(t,log(a3)) + multiply((1-t),log(1-a3)))\n",
    "    error = -error/n_examples\n",
    "    return error"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {
    "collapsed": true,
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "def numericalGradient(X, y, theta1, theta2, bias1, bias2):\n",
    "    epsilon = 10**-5\n",
    "    numGrad1     = np.zeros_like(theta1)\n",
    "    numGradBias1 = np.zeros_like(bias1)\n",
    "    numGrad2     = np.zeros_like(theta2)\n",
    "    numGradBias2 = np.zeros_like(bias2)\n",
    "    \n",
    "    for i in range(shape(theta1)[0]):\n",
    "        for j in range(shape(theta1)[1]):\n",
    "            theta1_pos = np.copy(theta1)\n",
    "            theta1_neg = np.copy(theta1)\n",
    "            theta1_pos[i,j] += epsilon\n",
    "            theta1_neg[i,j] -= epsilon\n",
    "            numGrad1[i,j]    = (errorFn(X, y, theta1_pos, theta2, bias1, bias2) - \\\n",
    "                                errorFn(X, y, theta1_neg, theta2, bias1, bias2))/2/epsilon\n",
    "    for i in range(shape(theta2)[0]):\n",
    "        for j in range(shape(theta2)[1]):\n",
    "            theta2_pos = np.copy(theta2)\n",
    "            theta2_neg = np.copy(theta2)\n",
    "            theta2_pos[i,j] += epsilon\n",
    "            theta2_neg[i,j] -= epsilon\n",
    "            numGrad2[i,j]    = (errorFn(X, y, theta1, theta2_pos, bias1, bias2) - \\\n",
    "                                errorFn(X, y, theta1, theta2_neg, bias1, bias2))/2/epsilon\n",
    "    for i in range(shape(bias1)[0]):\n",
    "        for j in range(shape(bias1)[1]):\n",
    "            bias1_pos         = np.copy(bias1)\n",
    "            bias1_neg         = np.copy(bias1)\n",
    "            bias1_pos[i,j]   += epsilon\n",
    "            bias1_neg[i,j]   -= epsilon\n",
    "            numGradBias1[i,j] = (errorFn(X, y, theta1, theta2, bias1_pos, bias2) - \\\n",
    "                                errorFn(X, y, theta1, theta2, bias1_neg, bias2))/2/epsilon\n",
    "    for i in range(shape(bias2)[0]):\n",
    "        for j in range(shape(bias2)[1]):\n",
    "            bias2_pos         = np.copy(bias2)\n",
    "            bias2_neg         = np.copy(bias2)\n",
    "            bias2_pos[i,j]   += epsilon\n",
    "            bias2_neg[i,j]   -= epsilon\n",
    "            numGradBias2[i,j] = (errorFn(X, y, theta1, theta2, bias1, bias2_pos) - \\\n",
    "                                errorFn(X, y, theta1, theta2, bias1, bias2_neg))/2/epsilon\n",
    "    return numGrad1,numGradBias1,numGrad2,numGradBias2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "sigmoid = lambda z: 1.0/(1.0+np.exp(-z))\n",
    "sigmoid_prime = lambda z: multiply(sigmoid(z),(1-sigmoid(z)))\n",
    "ftanh = lambda z: np.tanh(z)\n",
    "ftanh_prime = lambda z: 1 - multiply(ftanh(z),ftanh(z))\n",
    "funny_tanh = lambda z: 1.7159 * np.tanh(2.0/3.0 * z) + .001*z\n",
    "funny_tanh_prime = lambda z: 1.7159 * 2.0 / 3.0 * (1.0 / multiply(np.cosh(2.0/3.0 * z),np.cosh(2.0/3.0 * z))) + .001\n",
    "relu = lambda z: multiply(z,(z > 0))\n",
    "relu_prime = lambda z: z >= 0\n",
    "leaky_relu = lambda z: np.maximum(.1*z, z)\n",
    "leaky_relu_prime = lambda z: 1*(z>=0) + .1*(z<0)\n",
    "\n",
    "act_funcs = {'sigmoid': (sigmoid, sigmoid_prime),\n",
    "             'ftanh': (ftanh, ftanh_prime),\n",
    "             'funny_tanh': (funny_tanh, funny_tanh_prime),\n",
    "             'relu': (relu,relu_prime),\n",
    "             'leaky_relu': (leaky_relu,leaky_relu_prime)}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 2",
   "language": "python",
   "name": "python2"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.10"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 0
}
